<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>Clustering Report — Credit Card & Wine Quality</title>
  <style>
    body { font-family: system-ui, -apple-system, "Segoe UI", Roboto, "Helvetica Neue", Arial; line-height:1.6; margin: 24px; color:#111; }
    header { border-bottom: 1px solid #eee; padding-bottom: 12px; margin-bottom: 20px; }
    h1 { margin:0; font-size:28px; }
    h2 { color:#222; border-left:4px solid #6b7280; padding-left:10px; }
    section { margin-bottom: 18px; }
    pre { background:#f6f8fa; padding:12px; overflow:auto; border-radius:6px; }
    .grid { display:grid; grid-template-columns: 1fr 1fr; gap:16px; }
    .note { background:#fff7ed; border-left:4px solid #f59e0b; padding:10px; border-radius:6px; }
    .kbd { background:#eef2ff; border:1px solid #c7d2fe; padding:2px 6px; border-radius:4px; font-family:monospace; }
    footer { border-top:1px solid #eee; padding-top:12px; color:#555; }
    table { border-collapse:collapse; width:100%; }
    th, td { padding:8px 10px; border:1px solid #e6e6e6; text-align:left; }
    .center { text-align:center; }
  </style>
</head>
<body>
  <header>
    <h1>Clustering Analysis Report</h1>
    <p><strong>Datasets:</strong> Default of Credit Card Clients & Wine Quality (Red)</p>
    <p><em>Techniques:</em> K&#8209;Means Clustering and Hierarchical (Agglomerative) Clustering</p>
  </header>

  <section id="abstract">
    <h2>Abstract</h2>
    <p>
      This report summarizes unsupervised clustering analyses performed on two datasets: a credit card client dataset and the Wine Quality dataset.
      The primary goal was to discover natural groupings using K&#8209;Means and Hierarchical clustering, compare results across methods, and produce an interpretable summary (including a Y data profile for cluster labels).
    </p>
  </section>

  <section id="datasets">
    <h2>Datasets</h2>
    <p><strong>Default of Credit Card Clients</strong> — transactional and demographic features for credit card clients (credit limit, sex, age, payment history, etc.).</p>
    <p><strong>Wine Quality (Red)</strong> — physicochemical measurements for red wines and quality scores.</p>
  </section>

  <section id="methodology">
    <h2>Methodology</h2>
    <ol>
      <li><strong>Data import:</strong> data files loaded with pandas (<code>read_excel / read_csv</code>).</li>
      <li><strong>Feature selection:</strong> two numerical features chosen per experiment to enable 2D visualization (examples: LIMIT_BAL & SEX; fixed acidity & volatile acidity).</li>
      <li><strong>Optimal cluster number:</strong> K&#8209;Means used the elbow method (WCSS vs k). Hierarchical used a dendrogram (Ward linkage).</li>
      <li><strong>Training:</strong> K&#8209;Means with <code>n_clusters=5</code> and <code>k-means++</code>; AgglomerativeClustering with <code>n_clusters=5</code>, Euclidean distance and Ward linkage.</li>
      <li><strong>Visualization:</strong> scatter plots with cluster colors; centroids plotted for K&#8209;Means; dendrogram for hierarchical.</li>
      <li><strong>Y data profile:</strong> after clustering the produced labels (e.g. <code>y_kmeans</code>) were profiled for counts, balance and basic statistics per cluster.</li>
    </ol>
  </section>

  <section id="y-profile">
    <h2>Y Data Profile (general template)</h2>
    <p>The Y data profile summarizes the target variable (or pseudo-target after clustering). For clustering, Y equals the cluster label assigned by the algorithm.</p>
    <table>
      <thead><tr><th>Metric</th><th>Example / Description</th></tr></thead>
      <tbody>
        <tr><td>Type</td><td>Categorical (cluster id, integer)</td></tr>
        <tr><td>Number of clusters</td><td>5</td></tr>
        <tr><td>Counts per cluster</td><td>List of counts and percentages (e.g., Cluster 0: 1,200 (24%))</td></tr>
        <tr><td>Cluster overlap</td><td>Check with silhouette score or cross‑tab with known labels</td></tr>
        <tr><td>Representative stats</td><td>Mean/median of X features per cluster</td></tr>
        <tr><td>Suggested visuals</td><td>Bar chart (counts), boxplots per cluster, cluster heatmap</td></tr>
      </tbody>
    </table>
    <p class="note">Tip: For supervised tasks, the Y profile should also show class imbalance, missingness, and distribution shape.</p>
  </section>

  <section id="kmeans">
    <h2>K&#8209;Means Analysis</h2>
    <p><strong>Procedure:</strong> computed WCSS for k=1..10, selected k at elbow (here k=5), fitted model, extracted <code>y_kmeans</code>, and plotted clusters and centroids.</p>
    <pre><code># example (Python)
from sklearn.cluster import KMeans
kmeans = KMeans(n_clusters=5, init='k-means++', random_state=42)
y_kmeans = kmeans.fit_predict(X)</code></pre>
    <p><strong>Outputs to include in report:</strong></p>
    <ul>
      <li>Elbow plot (WCSS vs k)</li>
      <li>Scatter plot with clusters & centroids</li>
      <li>Cluster centroids table (mean feature values per cluster)</li>
      <li>Silhouette score or Davies–Bouldin index</li>
    </ul>
  </section>

  <section id="hierarchical">
    <h2>Hierarchical Clustering</h2>
    <p><strong>Procedure:</strong> built dendrogram (Ward linkage), chose cut level for 5 clusters, applied AgglomerativeClustering, visualized clusters.</p>
    <pre><code># example (Python)
import scipy.cluster.hierarchy as sch
dendrogram = sch.dendrogram(sch.linkage(X, method='ward'), truncate_mode='lastp', p=30)</code></pre>
    <p><strong>Outputs to include:</strong> Dendrogram figure, cluster scatter plot, cluster membership table.</p>
  </section>

  <section id="evaluations">
    <h2>Evaluation & Interpretation</h2>
    <p>Recommended evaluation metrics and diagnostics for clustering results:</p>
    <ul>
      <li><strong>Silhouette score</strong> — measures cohesion and separation (range -1..1).</li>
      <li><strong>Davies–Bouldin index</strong> — lower is better.</li>
      <li><strong>Cluster sizes</strong> — check imbalance that may affect downstream analysis.</li>
      <li><strong>Cross‑comparison</strong> — compare K&#8209;Means labels vs hierarchical labels with a contingency table.</li>
      <li><strong>Feature summaries</strong> — per-cluster means/medians to describe segments.</li>
    </ul>
  </section>

  <section id="recommendations">
    <h2>Recommendations</h2>
    <ol>
      <li>Use more features or PCA for higher-dimensional clustering (visualize with 2‑D PCA projection).</li>
      <li>Scale features before clustering (StandardScaler or MinMaxScaler).</li>
      <li>Run multiple initializations for K&#8209;Means and report variability.</li>
      <li>Use robust outlier handling (IQR capping or robust scaling) if cluster centers are affected.</li>
    </ol>
  </section>

  <section id="appendix">
    <h2>Appendix — How to reproduce</h2>
    <p>Save this report and run the notebooks provided. Basic commands:</p>
    <pre><code># in a Python environment
pip install -r requirements.txt
jupyter notebook
# open credit_card_kmeans.ipynb and wine_kmeans.ipynb</code></pre>
    <p class="center">Download the report from the project folder if needed.</p>
  </section>

  <footer>
    <p>Generated report template. Author: Dimakatso Kgare. Date: October 2025.</p>
  </footer>
</body>
</html>